{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e30ba6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efaa9bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu 4\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8772bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, size, posreward = 2, negreward = -2):\n",
    "        self.size = size\n",
    "        self.environment = np.array([[['0'] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        self.posreward = posreward\n",
    "        self.negreward = negreward\n",
    "        self.action_space = self.environment.shape[0]*self.environment.shape[1]*self.environment.shape[2]\n",
    "        self.observation_space = self.environment.shape[0]*self.environment.shape[1]\n",
    "        self.input_space = self.environment.shape[0],self.environment.shape[1]\n",
    "        \n",
    "    def resetField(self):\n",
    "        self.environment = np.array([[['0'] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        \n",
    "    def resetEnvironment(self):\n",
    "        self.resetField()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def GenerateEnvironment(self):\n",
    "        return np.array([[['0'] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "    \n",
    "    def genObs(self, env):\n",
    "        \n",
    "        \n",
    "        return np.array((env == '0').sum(axis=2), dtype=np.float32)\n",
    "    \n",
    "    def step(self, env, action):\n",
    "        #Save Old State\n",
    "        newState = env.copy()\n",
    "        actionPos = np.unravel_index(action, newState.shape)\n",
    "        done = False\n",
    "        \n",
    "        #Make move\n",
    "        if self.placeContainer(actionPos, newState):\n",
    "            #If move is allowed reward\n",
    "            newState[actionPos] = '#'\n",
    "            reward = self.posreward\n",
    "        else:\n",
    "            #If move is not allowed punish\n",
    "            reward = self.negreward\n",
    "        \n",
    "        #End game if field is all #s or if the user messed up.\n",
    "        if np.all(newState == '#') or reward == self.negreward:\n",
    "            done = True\n",
    "            self.done = True\n",
    "        return newState, reward, done\n",
    "    \n",
    "    def placeContainer(self, pos, env):\n",
    "        if self.isLegal(pos, env):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def isLegal(self, pos, env):\n",
    "        IO = self.isOccupied(pos, env)\n",
    "        IF = self.isFloating(pos, env)\n",
    "        IIE = self.posIsInEnv(pos, env)\n",
    "        NAS = self.hasNorthAndSouth(pos, env)\n",
    "        #print(IO,IF,IIE,NAS)\n",
    "        return not IO and not IF and IIE and not NAS\n",
    "    \n",
    "    def isOccupied(self, pos, env):\n",
    "        if self.posIsInEnv(pos, env):\n",
    "            return env[pos] == '#'\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def hasNorthAndSouth(self, pos, env):\n",
    "        NC = self.isOccupied((pos[0],pos[1]-1,0), env)\n",
    "        SC = self.isOccupied((pos[0],pos[1]+1,0), env)\n",
    "        #print(NC,SC)\n",
    "        return NC and SC\n",
    "    \n",
    "    def posIsInEnv(self, pos, env):\n",
    "        x = 0 <= pos[0] < env.shape[0]\n",
    "        y = 0 <= pos[1] < env.shape[1]\n",
    "        z = 0 <= pos[2] < env.shape[2]  \n",
    "        return x and y and z\n",
    "    \n",
    "    def isFloating(self, pos, env):\n",
    "        return np.any(env[pos[0],pos[1],:pos[2]] == '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b2202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  [0, 4, 2]  done\n",
      "Test  [1]  done\n",
      "Test  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]  done\n"
     ]
    }
   ],
   "source": [
    "#UnitTests\n",
    "Steps = [[0,4,2],[1],[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]]\n",
    "\n",
    "for o in Steps:\n",
    "    e = Environment(size=(3,3,2))\n",
    "    environment = e.GenerateEnvironment()\n",
    "    \n",
    "    for i in o:\n",
    "        environment, reward, done = e.step(environment,i)\n",
    "        #print(\"Position: \", np.unravel_index(i, environment.shape), \"\\nEnvironment: \", environment.flatten(), \"\\nReward\", reward)\n",
    "    print(\"Test \",o,\" done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b6bfd",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Agent(\n",
    "    -Environment\n",
    "    -CNN\n",
    "    -Trainer)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Pseudo pseudocode\n",
    "<code>N = 10</code>\n",
    "\n",
    "<code>While True:</code><br>\n",
    "<code>    //Gaming moment (play 10 games)</code><br>\n",
    "<code>    for i in range(10):</code><br>\n",
    "<code>        pred = agent.pred()</code><br>\n",
    "<code>        agent.rememberMoves()</code><br>\n",
    "<code>    //Learning moment (train on previous games if possible)</code><br>\n",
    "<code>    agent.trainNN()</code><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6f6c4",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cac1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, lr, input_space, action_space):\n",
    "        super().__init__()\n",
    "        self.input_space = input_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU())\n",
    "        self.fc = nn.Linear(16*np.prod(self.input_space), self.action_space)\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h = self.layer1(X)\n",
    "        h = h.reshape(-1, 16*np.prod(self.input_space))\n",
    "        y = self.fc(h)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2945ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, lr, batch_size, max_mem_size=100000, size = (3,3,2),epsilon = 0.99, eps_end=0.01, eps_dec=5e-4):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        \n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create Environment\n",
    "        self.env = Environment(size)\n",
    "        \n",
    "        #self.action_space = self.env.action_space\n",
    "        self.action_space = [i for i in range(self.env.action_space)]\n",
    "        \n",
    "        #Create Convolutional Neural Network\n",
    "        self.cnn = CNN(lr = lr, input_space = self.env.input_space, action_space = self.env.action_space)\n",
    "        \n",
    "        #Memory variables\n",
    "        self.mem_size = max_mem_size\n",
    "        self.mem_cntr = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Memory array variables\n",
    "        self.state_memory = np.zeros((self.mem_size, 1, *self.env.input_space), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, 1, *self.env.input_space), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "        \n",
    "    def chooseAction(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            #Turn observation into tensor\n",
    "            state = T.tensor([observation]).to(self.cnn.device)\n",
    "            #Get action from neural network\n",
    "            actions = self.cnn.forward(state)\n",
    "\n",
    "            #Get maximum value and return index\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "        \n",
    "    def rememberMoves(self, state, action, reward, state_, done):\n",
    "        #Makes counter loop over if max has been reached.\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        #Save specific variables to arrays\n",
    "        self.state_memory[index] = [state]\n",
    "        self.new_state_memory[index] = [state_]\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        #Increment memory counter\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    #TODO: We still need to add the trainer to this part.\n",
    "    def trainNN(self):\n",
    "        #If there's not enough memory for the batch size. Don't learn.\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.cnn.optimizer.zero_grad()\n",
    "        \n",
    "        #Check what the maximum size is of the memory\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        \n",
    "        #Choose a random batch\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        #Get batch indices for the batch size. (aka an array of batch_size int32s)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        #Load batch memory to device in tensors.\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.cnn.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.cnn.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.cnn.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.cnn.device)\n",
    "        \n",
    "    \n",
    "        \n",
    "        #Load action batch memory.\n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        #Generate output for eval state.\n",
    "        q_eval = self.cnn.forward(state_batch)[batch_index, action_batch]\n",
    "        \n",
    "        #Generate output for next state.\n",
    "        q_next = self.cnn.forward(new_state_batch)\n",
    "        \n",
    "        #Done state gets reset for the terminal batch?? idk?\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        \n",
    "        #Apply rewards to next batch.\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.cnn.loss(q_target, q_eval).to(self.cnn.device)\n",
    "        loss.backward()\n",
    "        self.cnn.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626db0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.68\n"
     ]
    }
   ],
   "source": [
    "a = Agent(0.3, 0.01, 128,size=(3,3,5))\n",
    "#env = a.env.GenerateEnvironment()\n",
    "\n",
    "\n",
    "RewardHistory = []\n",
    "RewardHistoryHistory = []\n",
    "#TOFIX: Check if memory is programmed right?\n",
    "#TOFIX: Allow the training step to actually work.\n",
    "\n",
    "done = False\n",
    "for o in range(100000):\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        a.env.resetEnvironment()\n",
    "        env = a.env.GenerateEnvironment()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = a.env.genObs(env)\n",
    "            action = a.chooseAction(obs)\n",
    "            newenv, reward, done = a.env.step(env,action)\n",
    "            \n",
    "            RewardHistory.append(reward)\n",
    "            \n",
    "            a.rememberMoves(a.env.genObs(env), action, reward, a.env.genObs(newenv), done)\n",
    "            env = newenv\n",
    "\n",
    "    a.trainNN()\n",
    "    \n",
    "    \n",
    "    if o%100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(np.mean(RewardHistory[-100:]))\n",
    "        #RewardHistoryHistory.append(np.mean(RewardHistory[-100:]))\n",
    "        #clear_output(wait=True)\n",
    "        #plt.plot(RewardHistoryHistory)\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979c383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
