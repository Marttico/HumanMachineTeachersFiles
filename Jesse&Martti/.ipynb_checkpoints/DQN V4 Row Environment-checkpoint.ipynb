{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30ba6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efaa9bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu 1\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8772bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: een keuze tussen bij 4 keuzes (bij 4x?x?)\n",
    "#Output: Nieuwe environment\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, size, posreward = 2, negreward = -2, maxPriority = 4):\n",
    "        self.size = size\n",
    "        self.environment = np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        self.posreward = posreward\n",
    "        self.negreward = negreward\n",
    "        self.action_space = self.environment.shape[0]\n",
    "        self.input_space = size[0]*(maxPriority)+1\n",
    "        self.priorityMax = maxPriority\n",
    "        \n",
    "    def resetField(self):\n",
    "        self.environment = np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        \n",
    "    def resetEnvironment(self):\n",
    "        self.resetField()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def GenerateEnvironment(self):\n",
    "        return np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        temp = np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        maxcont = self.size[0] * self.size[2]\n",
    "        for i in range(self.size[1]):\n",
    "            for o in range(np.random.randint(maxcont*0.65,maxcont)):\n",
    "                actionPos = self.checkValidPositionInRow(temp,i)\n",
    "                temp[actionPos] = 1\n",
    "        return temp\n",
    "    \n",
    "    def genObs(self, env, containerType = 1, flattened=True):\n",
    "        #Count amount of open containers per row.\n",
    "        outputlist = np.zeros((self.priorityMax,self.size[0]),dtype=np.float32)\n",
    "\n",
    "        for i in range(self.priorityMax):\n",
    "            outputlist[i] = (env == i).sum(axis=2).sum(axis=1)\n",
    "            \n",
    "        if flattened:\n",
    "            return np.concatenate((outputlist.flatten(), np.array([containerType],dtype=np.float32)))\n",
    "        else:\n",
    "            return [outputlist,containerType]\n",
    "    \n",
    "    def step(self, env, action, priority = 1):\n",
    "        #actionspace = y\n",
    "        \n",
    "        \n",
    "        #Save Old State\n",
    "        newState = env.copy()\n",
    "        actionPos = self.checkValidPositionInRow(env, action)\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        #Make move\n",
    "        if self.placeContainer(actionPos, newState):\n",
    "            newState[actionPos] = priority\n",
    "            reward = self.getRewardList(env, priority)[action]\n",
    "        else:\n",
    "            reward = -2\n",
    "            done = True\n",
    "            self.done = True\n",
    "\n",
    "        #End game if field is all filled.\n",
    "        if np.all(newState != 0):\n",
    "            done = True\n",
    "            self.done = True\n",
    "        return newState, reward, done\n",
    "    \n",
    "    def placeContainer(self, pos, env):\n",
    "        if self.isLegal(pos, env):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def isLegal(self, pos, env):\n",
    "        IO = self.isOccupied(pos, env)\n",
    "        IF = self.isFloating(pos, env)\n",
    "        IIE = self.posIsInEnv(pos, env)\n",
    "        NAS = self.hasNorthAndSouth(pos, env)\n",
    "        return not IO and not IF and IIE and not NAS\n",
    "    \n",
    "    def isOccupied(self, pos, env):\n",
    "        if self.posIsInEnv(pos, env):\n",
    "            return env[pos] != 0\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def hasNorthAndSouth(self, pos, env):\n",
    "        NC = self.isOccupied((pos[0],pos[1]-1,0), env)\n",
    "        SC = self.isOccupied((pos[0],pos[1]+1,0), env)\n",
    "        return NC and SC\n",
    "    \n",
    "    def posIsInEnv(self, pos, env):\n",
    "        x = 0 <= pos[0] < env.shape[0]\n",
    "        y = 0 <= pos[1] < env.shape[1]\n",
    "        z = 0 <= pos[2] < env.shape[2]  \n",
    "        return x and y and z\n",
    "    \n",
    "    def isFloating(self, pos, env):\n",
    "        return np.any(env[pos[0],pos[1],:pos[2]] == 0)\n",
    "    \n",
    "    def checkValidPositionInRow(self, env, row):\n",
    "        positions = np.dstack(np.where(env[row,:,:] == 0))\n",
    "        if positions.size != 0:\n",
    "            result = positions[positions[:,:,0] == np.max(positions[:,:,0])][0]\n",
    "            \n",
    "        else:\n",
    "            result = (0,0)\n",
    "        return row, result[0], result[1]\n",
    "    \n",
    "    def getRewardList(self,env,containerPriority):\n",
    "        obs = self.genObs(env,containerPriority,flattened=False)\n",
    "        multarr = np.full(obs[0].shape, -1)\n",
    "        multarr[containerPriority] *= -1\n",
    "        multarr[0] *= 0\n",
    "        obs[0] *= multarr\n",
    "        obs[0] *= obs[0] != np.prod(env.shape[1:3])\n",
    "        #add that it gets punished if it puts a container on a full stack.\n",
    "        return obs[0].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7744ffb4",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3., -20.,   0., -25.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reward: When putting same type containers \n",
    "\n",
    "env = Environment(size=(5,5,5))\n",
    "\n",
    "testenv = np.empty((5,5,5))\n",
    "testenv[0,0,0] = 1\n",
    "testenv[0,1,0] = 1\n",
    "testenv[0,2,0] = 1\n",
    "\n",
    "testenv[1,:,:-1] = 3\n",
    "testenv[3,:,:] = 2\n",
    "containerpriority = 1\n",
    "action = 1\n",
    "\n",
    "env.genObs(testenv,10)\n",
    "env.getRewardList(testenv,containerpriority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1578bbf9",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 6. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 12\n",
      "[6. 6. 6. 0. 0. 0. 0. 0. 0. 0. 0. 0.] 12\n"
     ]
    }
   ],
   "source": [
    "#UnitTests\n",
    "for o in range(2):\n",
    "    e = Environment(size=(3,3,2))\n",
    "    environment = e.GenerateEnvironment()\n",
    "    print(e.genObs(environment),e.input_space)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b2202c",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 0)\n",
      "Action:  2 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]] \n",
      "Reward 2\n",
      "False\n",
      "(2, 2, 1)\n",
      "Action:  2 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 0]]] \n",
      "Reward 2\n",
      "False\n",
      "(1, 2, 0)\n",
      "Action:  1 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 0]]] \n",
      "Reward 2\n",
      "False\n",
      "(0, 2, 0)\n",
      "Action:  0 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 0]]] \n",
      "Reward 2\n",
      "False\n",
      "(0, 2, 1)\n",
      "Action:  0 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 0]]] \n",
      "Reward 2\n",
      "False\n",
      "(2, 2, 2)\n",
      "Action:  2 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 1]]] \n",
      "Reward 2\n",
      "False\n",
      "(0, 2, 2)\n",
      "Action:  0 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 1]]] \n",
      "Reward 2\n",
      "False\n",
      "(0, 1, 0)\n",
      "Action:  0 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [1 0 0]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 1]]] \n",
      "Reward 2\n",
      "False\n",
      "(0, 1, 1)\n",
      "Action:  0 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [1 1 0]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 1]]] \n",
      "Reward 2\n",
      "False\n",
      "(1, 2, 1)\n",
      "Action:  1 \n",
      "Environment: \n",
      " [[[0 0 0]\n",
      "  [1 1 0]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [1 1 1]]] \n",
      "Reward 2\n",
      "False\n",
      "Test  [2, 2, 1, 0, 0, 2, 0, 0, 0, 1]  done\n",
      " [4. 7. 6.]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#UnitTests\n",
    "\n",
    "Steps = [[2,2,1,0,0,2,0,0,0,1]]\n",
    "\n",
    "for o in Steps:\n",
    "    e = Environment(size=(3,3,3))\n",
    "    environment = e.GenerateEnvironment()\n",
    "    \n",
    "    for i in o:\n",
    "        environment, reward, done = e.step(environment,i)\n",
    "        print(\"Action: \",i, \"\\nEnvironment: \\n\", environment, \"\\nReward\", reward)\n",
    "        print(done)\n",
    "    print(\"Test \",o,\" done\\n\",e.genObs(environment))\n",
    "    print(e.input_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b6bfd",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Agent(\n",
    "    -Environment\n",
    "    -CNN\n",
    "    -Trainer)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Pseudo pseudocode\n",
    "<code>N = 10</code>\n",
    "\n",
    "<code>While True:</code><br>\n",
    "<code>    //Gaming moment (play 10 games)</code><br>\n",
    "<code>    for i in range(10):</code><br>\n",
    "<code>        pred = agent.pred()</code><br>\n",
    "<code>        agent.rememberMoves()</code><br>\n",
    "<code>    //Learning moment (train on previous games if possible)</code><br>\n",
    "<code>    agent.trainNN()</code><br>\n",
    "\n",
    "TODO: INCLUDE SCORING\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6f6c4",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cac1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, n_actions, fc1_dims=64, fc2_dims=64):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2945ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, lr, batch_size, max_mem_size=100000, size = (3,3,2),epsilon = 0.99, eps_end=0.01, eps_dec=5e-3):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        \n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create Environment\n",
    "        self.env = Environment(size)\n",
    "        \n",
    "        #self.action_space = self.env.action_space\n",
    "        self.action_space = [i for i in range(self.env.action_space)]\n",
    "        \n",
    "        #Create Convolutional Neural Network\n",
    "        self.Q_eval = DeepQNetwork(lr = lr, input_dims = self.env.input_space, n_actions = self.env.action_space)\n",
    "        \n",
    "        #Memory variables\n",
    "        self.mem_size = max_mem_size\n",
    "        self.mem_cntr = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Memory array variables\n",
    "        self.state_memory = np.zeros((self.mem_size, self.env.input_space), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, self.env.input_space), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "        \n",
    "    def chooseAction(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            #Turn observation into tensor\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            #Get action from neural network\n",
    "            actions = self.Q_eval.forward(state)\n",
    "\n",
    "            #Get maximum value and return index\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "        \n",
    "    def rememberMoves(self, state, action, reward, state_, done):\n",
    "        #Makes counter loop over if max has been reached.\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        #Save specific variables to arrays\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        #Increment memory counter\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    #TODO: We still need to add the trainer to this part.\n",
    "    def trainNN(self):\n",
    "        #If there's not enough memory for the batch size. Don't learn.\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        #Check what the maximum size is of the memory\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        \n",
    "        #Choose a random batch\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        #Get batch indices for the batch size. (aka an array of batch_size int32s)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        #Load batch memory to device in tensors.\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "    \n",
    "        \n",
    "        #Load action batch memory.\n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        #Generate output for eval state.\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        \n",
    "        #Generate output for next state.\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        \n",
    "        #Done state gets reset for the terminal batch?? idk?\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        #Apply rewards to next batch.\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1626db0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 61230/100000 score 7.92 epsilon 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m boardObservations\u001b[38;5;241m.\u001b[39mappend(env)\n\u001b[1;32m     33\u001b[0m obs \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgenObs(env,priorityList[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 35\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m newenv, reward, done \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(env,action,priority\u001b[38;5;241m=\u001b[39mpriorityList[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     39\u001b[0m priorityList \u001b[38;5;241m=\u001b[39m priorityList[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn [5], line 41\u001b[0m, in \u001b[0;36mAgent.chooseAction\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     38\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_eval\u001b[38;5;241m.\u001b[39mforward(state)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m#Get maximum value and return index\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = Agent(0.3, 0.001, 64, size=(4, 4, 5))\n",
    "#env = a.env.GenerateEnvironment()\n",
    "\n",
    "#Geen prioriteit: Randomly invullen en kijken wat ie doet :)\n",
    "#Wel prioriteit: Randomly invullen en voeg score van de move toe.\n",
    "\n",
    "\n",
    "RewardHistory = []\n",
    "RewardHistoryHistory = []\n",
    "#TOFIX: Check if memory is programmed right?\n",
    "#TOFIX: Allow the training step to actually work.\n",
    "\n",
    "done = False\n",
    "n_games = 100000\n",
    "N = 10\n",
    "\n",
    "\n",
    "lastObservation = 0\n",
    "\n",
    "for o in range(n_games):\n",
    "    \n",
    "    nhistory = []\n",
    "    for i in range(N):\n",
    "        a.env.resetEnvironment()\n",
    "        env = a.env.GenerateEnvironment()\n",
    "        priorityList = np.random.randint(1,a.env.priorityMax,size=(np.prod(env.size)))\n",
    "        done = False\n",
    "        episodescore = []\n",
    "        boardObservations = []\n",
    "        while not done:\n",
    "            boardObservations.append(env)\n",
    "            \n",
    "            obs = a.env.genObs(env,priorityList[-1])\n",
    "            \n",
    "            action = a.chooseAction(obs)\n",
    "            \n",
    "            newenv, reward, done = a.env.step(env,action,priority=priorityList[-1])\n",
    "            \n",
    "            priorityList = priorityList[:-1]\n",
    "            \n",
    "            a.rememberMoves(obs, action, reward, a.env.genObs(newenv), done)\n",
    "            \n",
    "            env = newenv\n",
    "            episodescore.append(reward)\n",
    "        \n",
    "        if(o%100 == 0 and i == 0):\n",
    "            np.save('Observations/array%i.npy'%(o),boardObservations)\n",
    "        \n",
    "        nhistory.append(np.mean(episodescore))\n",
    "    a.trainNN()\n",
    "    RewardHistory.append(np.mean(nhistory))\n",
    "    \n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(\"Episode %i/%i score %.2f epsilon %.2f\"%(o*N,n_games,RewardHistory[-1],a.epsilon),flush=True)\n",
    "    #print(RewardHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RewardHistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b7245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3)[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1ea3f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(priorityList[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualisation.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-1, 1e-4, log=True)\n",
    "    \n",
    "    # process of training model\n",
    "    model = CNN(lr = lr, )\n",
    "    t = Trainer()\n",
    "    t.train\n",
    "    \n",
    "    return #score\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(trial, n_trials= 100, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.Q_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c84bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f74f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
