{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30ba6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "#from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efaa9bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu 2\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "156e0f04",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 9])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testenv = np.array([[[0] * 5] * 4] * 3)\n",
    "testenv[1,3,0] = 1\n",
    "testenv[2,3,1] = 1\n",
    "testenv[2,3,2] = 1\n",
    "testenv[2,3,3] = 1\n",
    "testenv[2,3,4] = 1\n",
    "testenv[2,2,0] = 0\n",
    "testenv[2,1,0] = 1\n",
    "testenv[2,1,1] = 1\n",
    "testenv[2,1,2] = 1\n",
    "testenv[2,2,3] = 1\n",
    "testenv[2,2,4] = 1\n",
    "testenv[2,2,0] = 0\n",
    "\n",
    "testenv.sum(axis=2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: een keuze tussen bij 4 keuzes (bij 4x?x?)\n",
    "#Output: Nieuwe environment\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, size, posreward = 2, negreward = -2):\n",
    "        self.size = size\n",
    "        self.environment = np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        self.posreward = posreward\n",
    "        self.negreward = negreward\n",
    "        self.action_space = self.environment.shape[0]\n",
    "        self.observation_space = np.prod(size)\n",
    "        self.input_space = size[0]\n",
    "        \n",
    "    def resetField(self):\n",
    "        self.environment = np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        \n",
    "    def resetEnvironment(self):\n",
    "        self.resetField()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "    \n",
    "    def GenerateEnvironment(self):\n",
    "        return np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        temp = np.array([[[0] * self.size[2]] * self.size[1]] * self.size[0])\n",
    "        maxcont = self.size[0] * self.size[2]\n",
    "        for i in range(self.size[1]):\n",
    "            for o in range(np.random.randint(maxcont*0.65,maxcont)):\n",
    "                actionPos = self.checkValidPositionInRow(temp,i)\n",
    "                temp[actionPos] = 1\n",
    "        return temp\n",
    "    \n",
    "    def genObs(self, env):\n",
    "        #Count amount of open containers per row.\n",
    "        #env.sum(axis=2).sum(axis=0)\n",
    "        \n",
    "        return np.array((env == 0).sum(axis=2).sum(axis=1), dtype=np.float32)\n",
    "        #return np.array((env == 0).sum(axis=2), dtype=np.float32)\n",
    "    \n",
    "    def step(self, env, action):\n",
    "        #actionspace = y\n",
    "        \n",
    "        \n",
    "        #Save Old State\n",
    "        newState = env.copy()\n",
    "        \n",
    "        actionPos = self.checkValidPositionInRow(env, action)\n",
    "        #print(actionPos)\n",
    "        done = False\n",
    "        \n",
    "        #Make move\n",
    "        if self.placeContainer(actionPos, newState):\n",
    "            #If move is allowed reward\n",
    "            newState[actionPos] = 1\n",
    "            reward = self.posreward\n",
    "        else:\n",
    "            #If move is not allowed punish\n",
    "            reward = self.negreward\n",
    "        \n",
    "        #End game if field is all #s or if the user messed up.\n",
    "        if np.all(newState != 0) or reward == self.negreward:\n",
    "            done = True\n",
    "            self.done = True\n",
    "        return newState, reward, done\n",
    "    \n",
    "    def placeContainer(self, pos, env):\n",
    "        if self.isLegal(pos, env):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def isLegal(self, pos, env):\n",
    "        IO = self.isOccupied(pos, env)\n",
    "        IF = self.isFloating(pos, env)\n",
    "        IIE = self.posIsInEnv(pos, env)\n",
    "        NAS = self.hasNorthAndSouth(pos, env)\n",
    "        #print(IO,IF,IIE,NAS)\n",
    "        return not IO and not IF and IIE and not NAS\n",
    "    \n",
    "    def isOccupied(self, pos, env):\n",
    "        if self.posIsInEnv(pos, env):\n",
    "            return env[pos] != 0\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def hasNorthAndSouth(self, pos, env):\n",
    "        NC = self.isOccupied((pos[0]-1,pos[1],0), env)\n",
    "        SC = self.isOccupied((pos[0]+1,pos[1],0), env)\n",
    "        #print(NC,SC)\n",
    "        return NC and SC\n",
    "    \n",
    "    def posIsInEnv(self, pos, env):\n",
    "        x = 0 <= pos[0] < env.shape[0]\n",
    "        y = 0 <= pos[1] < env.shape[1]\n",
    "        z = 0 <= pos[2] < env.shape[2]  \n",
    "        return x and y and z\n",
    "    \n",
    "    def isFloating(self, pos, env):\n",
    "        return np.any(env[pos[0],pos[1],:pos[2]] == 0)\n",
    "    \n",
    "    def checkValidPositionInRow(self, env, row):\n",
    "        positions = np.dstack(np.where(env[row,:,:] == 0))\n",
    "        if positions.size != 0:\n",
    "            result = positions[positions[:,:,0] == np.max(positions[:,:,0])][0]\n",
    "            \n",
    "        else:\n",
    "            result = (0,0)\n",
    "        return row, result[0], result[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1578bbf9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 0]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 1]\n",
      "  [1 1]]] [1. 3. 2.]\n",
      "[[[0 0]\n",
      "  [1 0]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 0]\n",
      "  [1 1]]] [3. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "#UnitTests\n",
    "for o in range(2):\n",
    "    e = Environment(size=(3,3,2))\n",
    "    environment = e.GenerateEnvironment()\n",
    "    print(environment, e.genObs(environment))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1b2202c",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  [0, 2, 2]  done\n",
      " [5. 6. 4.]\n",
      "(3, 3, 2)\n",
      "Test  [0, 0, 0, 1, 2]  done\n",
      " [3. 5. 5.]\n",
      "(3, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "#UnitTests\n",
    "Steps = [[0,2,2],[0,0,0,1,2]]\n",
    "\n",
    "for o in Steps:\n",
    "    e = Environment(size=(3,3,2))\n",
    "    environment = e.GenerateEnvironment()\n",
    "    \n",
    "    for i in o:\n",
    "        environment, reward, done = e.step(environment,i)\n",
    "        #print(\"Position: \", np.unravel_index(i, environment.shape), \"\\nEnvironment: \", environment.flatten(), \"\\nReward\", reward)\n",
    "    \n",
    "    print(\"Test \",o,\" done\\n\",e.genObs(environment))\n",
    "    print(e.input_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b6bfd",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Agent(\n",
    "    -Environment\n",
    "    -CNN\n",
    "    -Trainer)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Pseudo pseudocode\n",
    "<code>N = 10</code>\n",
    "\n",
    "<code>While True:</code><br>\n",
    "<code>    //Gaming moment (play 10 games)</code><br>\n",
    "<code>    for i in range(10):</code><br>\n",
    "<code>        pred = agent.pred()</code><br>\n",
    "<code>        agent.rememberMoves()</code><br>\n",
    "<code>    //Learning moment (train on previous games if possible)</code><br>\n",
    "<code>    agent.trainNN()</code><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6f6c4",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cac1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, n_actions, fc1_dims=32, fc2_dims=32):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2945ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, lr, batch_size, max_mem_size=100000, size = (3,3,2),epsilon = 0.99, eps_end=0.01, eps_dec=5e-3):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        \n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #Create Environment\n",
    "        self.env = Environment(size)\n",
    "        \n",
    "        #self.action_space = self.env.action_space\n",
    "        self.action_space = [i for i in range(self.env.action_space)]\n",
    "        \n",
    "        #Create Convolutional Neural Network\n",
    "        self.Q_eval = DeepQNetwork(lr = lr, input_dims = self.env.input_space, n_actions = self.env.action_space)\n",
    "        \n",
    "        #Memory variables\n",
    "        self.mem_size = max_mem_size\n",
    "        self.mem_cntr = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Memory array variables\n",
    "        self.state_memory = np.zeros((self.mem_size, self.env.input_space), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, self.env.input_space), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
    "        \n",
    "    def chooseAction(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            #Turn observation into tensor\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            #Get action from neural network\n",
    "            actions = self.Q_eval.forward(state)\n",
    "\n",
    "            #Get maximum value and return index\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "        \n",
    "    def rememberMoves(self, state, action, reward, state_, done):\n",
    "        #Makes counter loop over if max has been reached.\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        #Save specific variables to arrays\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        #Increment memory counter\n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    #TODO: We still need to add the trainer to this part.\n",
    "    def trainNN(self):\n",
    "        #If there's not enough memory for the batch size. Don't learn.\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        #Check what the maximum size is of the memory\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        \n",
    "        #Choose a random batch\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        #Get batch indices for the batch size. (aka an array of batch_size int32s)\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        #Load batch memory to device in tensors.\n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "        \n",
    "    \n",
    "        \n",
    "        #Load action batch memory.\n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        #Generate output for eval state.\n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        \n",
    "        #Generate output for next state.\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        \n",
    "        #Done state gets reset for the terminal batch?? idk?\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        #Apply rewards to next batch.\n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1626db0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12000/100000 score 1.24 epsilon 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 25\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenObs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     action \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mchooseAction(obs)\n\u001b[1;32m     27\u001b[0m     newenv, reward, done \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(env,action)\n",
      "Cell \u001b[0;32mIn [3], line 37\u001b[0m, in \u001b[0;36mEnvironment.genObs\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenObs\u001b[39m(\u001b[38;5;28mself\u001b[39m, env):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#Count amount of open containers per row.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#env.sum(axis=2).sum(axis=0)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = Agent(0.3, 0.01, 32, size=(3, 3, 2))\n",
    "#env = a.env.GenerateEnvironment()\n",
    "\n",
    "#Geen prioriteit: Randomly invullen en kijken wat ie doet :)\n",
    "#Wel prioriteit: Randomly invullen en voeg score van de move toe.\n",
    "\n",
    "\n",
    "RewardHistory = []\n",
    "RewardHistoryHistory = []\n",
    "#TOFIX: Check if memory is programmed right?\n",
    "#TOFIX: Allow the training step to actually work.\n",
    "\n",
    "done = False\n",
    "n_games = 100000\n",
    "N = 10\n",
    "\n",
    "for o in range(n_games):\n",
    "    \n",
    "    for i in range(N):\n",
    "        \n",
    "        a.env.resetEnvironment()\n",
    "        env = a.env.GenerateEnvironment()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs = a.env.genObs(env)\n",
    "            action = a.chooseAction(obs)\n",
    "            newenv, reward, done = a.env.step(env,action)\n",
    "            \n",
    "            RewardHistory.append(reward)\n",
    "            #print(env)\n",
    "            a.rememberMoves(a.env.genObs(env), action, reward, a.env.genObs(newenv), done)\n",
    "            env = newenv\n",
    "\n",
    "    a.trainNN()\n",
    "    \n",
    "    \n",
    "    if o%10 == 0:\n",
    "        clear_output(wait=True)\n",
    "        #print(o, np.mean(RewardHistory[-100:]))\n",
    "        \n",
    "        print(\"Episode %i/%i score %.2f epsilon %.2f\"%(o*N,n_games,np.mean(RewardHistory[-100:]),a.epsilon),flush=True)\n",
    "        RewardHistoryHistory.append(np.mean(RewardHistory[-100:]))\n",
    "        #clear_output(wait=True)\n",
    "        #plt.plot(RewardHistoryHistory)\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(trial):\n",
    "    lr = trial.suggest_float('lr', 1e-1, 1e-4, log=True)\n",
    "    \n",
    "    # process of training model\n",
    "    model = CNN(lr = lr, )\n",
    "    t = Trainer()\n",
    "    t.train\n",
    "    \n",
    "    return #score\n",
    "\n",
    "study = optuna.create_study(direction = 'maximize')\n",
    "study.optimize(trial, n_trials= 100, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66be90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualisation.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(RewardHistoryHistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.Q_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c84bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
